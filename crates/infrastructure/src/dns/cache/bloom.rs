use rustc_hash::FxHasher;
use std::hash::{Hash, Hasher};
use std::sync::atomic::{AtomicU64, Ordering as AtomicOrdering};

pub struct AtomicBloom {
    bits: Vec<AtomicU64>,
    /// Bitmask = `num_bits - 1` where `num_bits` is always a power of two.
    /// `nth_hash` uses `& mask` instead of `% num_bits`, replacing integer
    /// division (~30–40 cycles) with a single AND (~1 cycle).
    mask: u64,
    num_hashes: usize,
}

impl AtomicBloom {
    pub fn new(capacity: usize, fp_rate: f64) -> Self {
        let num_bits = Self::optimal_num_bits(capacity, fp_rate);
        let num_hashes = Self::optimal_num_hashes(capacity, num_bits);
        let num_words = num_bits.div_ceil(64);
        let bits = (0..num_words).map(|_| AtomicU64::new(0)).collect();
        Self {
            bits,
            mask: (num_bits as u64) - 1,
            num_hashes,
        }
    }

    #[inline]
    pub fn check<K: Hash>(&self, key: &K) -> bool {
        let (h1, h2) = Self::double_hash(key);
        let num_hashes = self.num_hashes;
        let mask = self.mask;

        if num_hashes == 5 {
            // Short-circuit evaluation: on a cache miss (the common rejection case)
            // we can return false after the very first zero bit, saving up to 4
            // atomic loads (~3 ns each).  The original code evaluated all 5 and
            // then ANDed — fast for hits but wasteful for misses.
            let idx0 = Self::nth_hash(h1, h2, 0, mask);
            if self.bits[idx0 / 64].load(AtomicOrdering::Relaxed) & (1u64 << (idx0 % 64)) == 0 {
                return false;
            }

            let idx1 = Self::nth_hash(h1, h2, 1, mask);
            if self.bits[idx1 / 64].load(AtomicOrdering::Relaxed) & (1u64 << (idx1 % 64)) == 0 {
                return false;
            }

            let idx2 = Self::nth_hash(h1, h2, 2, mask);
            if self.bits[idx2 / 64].load(AtomicOrdering::Relaxed) & (1u64 << (idx2 % 64)) == 0 {
                return false;
            }

            let idx3 = Self::nth_hash(h1, h2, 3, mask);
            if self.bits[idx3 / 64].load(AtomicOrdering::Relaxed) & (1u64 << (idx3 % 64)) == 0 {
                return false;
            }

            let idx4 = Self::nth_hash(h1, h2, 4, mask);
            self.bits[idx4 / 64].load(AtomicOrdering::Relaxed) & (1u64 << (idx4 % 64)) != 0
        } else {
            for i in 0..num_hashes {
                let bit_idx = Self::nth_hash(h1, h2, i as u64, mask);
                let word_idx = bit_idx / 64;
                let bit_pos = bit_idx % 64;
                if (self.bits[word_idx].load(AtomicOrdering::Relaxed) & (1u64 << bit_pos)) == 0 {
                    return false;
                }
            }
            true
        }
    }

    #[inline]
    pub fn set<K: Hash>(&self, key: &K) {
        let (h1, h2) = Self::double_hash(key);
        let num_hashes = self.num_hashes;
        let mask = self.mask;

        if num_hashes == 5 {
            let idx0 = Self::nth_hash(h1, h2, 0, mask);
            self.bits[idx0 / 64].fetch_or(1u64 << (idx0 % 64), AtomicOrdering::Relaxed);

            let idx1 = Self::nth_hash(h1, h2, 1, mask);
            self.bits[idx1 / 64].fetch_or(1u64 << (idx1 % 64), AtomicOrdering::Relaxed);

            let idx2 = Self::nth_hash(h1, h2, 2, mask);
            self.bits[idx2 / 64].fetch_or(1u64 << (idx2 % 64), AtomicOrdering::Relaxed);

            let idx3 = Self::nth_hash(h1, h2, 3, mask);
            self.bits[idx3 / 64].fetch_or(1u64 << (idx3 % 64), AtomicOrdering::Relaxed);

            let idx4 = Self::nth_hash(h1, h2, 4, mask);
            self.bits[idx4 / 64].fetch_or(1u64 << (idx4 % 64), AtomicOrdering::Relaxed);
        } else {
            for i in 0..num_hashes {
                let bit_idx = Self::nth_hash(h1, h2, i as u64, mask);
                let word_idx = bit_idx / 64;
                let bit_pos = bit_idx % 64;
                self.bits[word_idx].fetch_or(1u64 << bit_pos, AtomicOrdering::Relaxed);
            }
        }
    }

    pub fn clear(&self) {
        for word in &self.bits {
            word.store(0, AtomicOrdering::Relaxed);
        }
    }

    #[inline]
    fn double_hash<K: Hash>(key: &K) -> (u64, u64) {
        let mut hasher = FxHasher::default();
        key.hash(&mut hasher);
        let h1 = hasher.finish();
        let h2 = h1.wrapping_mul(0x517cc1b727220a95).rotate_right(17);
        (h1, h2)
    }

    /// Compute the n-th probe index using a pre-computed bitmask.
    ///
    /// `mask` must equal `num_bits - 1` where `num_bits` is a power of two.
    /// The `& mask` replaces the old `% num_bits` (integer division, ~30–40
    /// cycles) with a single bitwise AND (~1 cycle), saving ~150 cycles on a
    /// full 5-probe check.
    #[inline]
    fn nth_hash(h1: u64, h2: u64, n: u64, mask: u64) -> usize {
        (h1.wrapping_add(n.wrapping_mul(h2)) & mask) as usize
    }

    /// Returns the optimal bloom filter size **rounded up to the next power of
    /// two**.  Rounding up means `num_bits >= m_optimal`, so the actual FP rate
    /// is ≤ the requested `fp_rate` — correctness is preserved.
    fn optimal_num_bits(capacity: usize, fp_rate: f64) -> usize {
        let n = capacity as f64;
        let p = fp_rate;
        let m = (-(n * p.ln()) / (2.0_f64.ln().powi(2))).ceil() as usize;
        // Round up to a power of two so the bitmask trick in nth_hash works.
        m.next_power_of_two()
    }

    fn optimal_num_hashes(capacity: usize, num_bits: usize) -> usize {
        let n = capacity as f64;
        let m = num_bits as f64;
        ((m / n) * 2.0_f64.ln()).ceil() as usize
    }
}
