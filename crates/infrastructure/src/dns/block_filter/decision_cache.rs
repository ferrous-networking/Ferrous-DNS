use crate::dns::cache::coarse_clock::coarse_now_secs;
use ahash::RandomState as AHashRandomState;
use dashmap::DashMap;
use ferrous_dns_domain::BlockSource;
use lru::LruCache;
use rustc_hash::FxBuildHasher;
use std::cell::RefCell;
use std::hash::{BuildHasher, Hash, Hasher};
use std::num::NonZeroUsize;
use std::sync::atomic::{AtomicUsize, Ordering as AtomicOrdering};
use std::sync::OnceLock;

const TTL_SECS: u64 = 60;
const L0_CAPACITY: usize = 256;
const L1_CAPACITY: usize = 100_000;

/// Cache encoding: 0 = allow, 1 = Blocklist, 2 = ManagedDomain, 3 = RegexFilter
const CACHE_ALLOW: u8 = 0;

fn encode_source(source: Option<BlockSource>) -> u8 {
    match source {
        None => CACHE_ALLOW,
        Some(s) => s.as_u8() + 1,
    }
}

fn decode_source(val: u8) -> Option<BlockSource> {
    if val == CACHE_ALLOW {
        None
    } else {
        BlockSource::from_u8(val - 1)
    }
}

/// Fixed-seed ahash state shared across all threads.
///
/// Using `ahash::RandomState` with AES-NI gives 30–50% faster hashing
/// than `FxHasher` for domain strings > 16 bytes.  Fixed seeds ensure
/// all threads produce identical hashes for the same (domain, group_id)
/// pair — required for L0 ↔ L1 cache consistency.
static DECISION_HASH_STATE: OnceLock<AHashRandomState> = OnceLock::new();

#[inline]
fn decision_hash_state() -> &'static AHashRandomState {
    DECISION_HASH_STATE.get_or_init(|| {
        AHashRandomState::with_seeds(
            0xf4a5_f3e1_c2b0_a9d7,
            0x8e6b_4c2a_0f1d_e3c9,
            0x7a2c_1e5b_9d4f_6a8e,
            0x3c7a_2e4b_6f8d_0a1c,
        )
    })
}

/// Compute the combined hash key for a (domain, group_id) pair.
/// Exposed so callers (e.g. `engine.rs::check`) can compute it once and
/// reuse it across all L0 / L1 lookups instead of hashing 4 times.
#[inline]
pub fn decision_key(domain: &str, group_id: i64) -> u64 {
    let mut h = decision_hash_state().build_hasher();
    domain.hash(&mut h);
    group_id.hash(&mut h);
    h.finish()
}

// (encoded_source, timestamp)
type BlockL0Cache = LruCache<u64, (u8, u64), FxBuildHasher>;

thread_local! {
    static BLOCK_L0: RefCell<BlockL0Cache> =
        RefCell::new(LruCache::with_hasher(
            NonZeroUsize::new(L0_CAPACITY).unwrap(),
            FxBuildHasher,
        ));
}

/// Returns `None` on cache miss, `Some(None)` for cached allow, `Some(Some(source))` for cached block.
/// Accepts a pre-computed key from [`decision_key`] — compute the key once
/// and reuse it across all L0 / L1 lookups.
#[inline]
pub fn decision_l0_get_by_key(key: u64) -> Option<Option<BlockSource>> {
    BLOCK_L0.with(|c| {
        let mut c = c.borrow_mut();
        if let Some(&(encoded, inserted_at)) = c.get(&key) {
            if coarse_now_secs().saturating_sub(inserted_at) < TTL_SECS {
                return Some(decode_source(encoded));
            }
            c.pop(&key);
        }
        None
    })
}

/// Store a decision in the L0 thread-local cache.
/// Accepts a pre-computed key from [`decision_key`].
#[inline]
pub fn decision_l0_set_by_key(key: u64, source: Option<BlockSource>) {
    BLOCK_L0.with(|c| {
        c.borrow_mut()
            .put(key, (encode_source(source), coarse_now_secs()));
    });
}

pub fn decision_l0_clear() {
    BLOCK_L0.with(|c| c.borrow_mut().clear());
}

/// Shared L1 block-decision cache backed by a lock-free `DashMap`.
///
/// Replaces the previous `Mutex<LruCache>` design.  Since every entry carries
/// a TTL timestamp, strict LRU ordering is not required for correctness: stale
/// entries are rejected on read and overwritten on the next write.  The
/// `DashMap` lets multiple tokio worker threads read and write concurrently
/// without serialising through a single global mutex.
///
/// When the map exceeds `L1_CAPACITY` entries, new inserts are dropped to
/// bound memory.  The TTL (60 s) ensures natural turnover so the map does not
/// fill with stale data under normal load.
pub struct BlockDecisionCache {
    inner: DashMap<u64, (u8, u64), FxBuildHasher>,
    len: AtomicUsize,
}

impl BlockDecisionCache {
    pub fn new() -> Self {
        Self {
            inner: DashMap::with_capacity_and_hasher(L1_CAPACITY, FxBuildHasher),
            len: AtomicUsize::new(0),
        }
    }

    /// Returns `None` on cache miss, `Some(None)` for cached allow, `Some(Some(source))` for cached block.
    /// Accepts a pre-computed key from [`decision_key`].
    #[inline]
    pub fn get_by_key(&self, key: u64) -> Option<Option<BlockSource>> {
        if let Some(entry) = self.inner.get(&key) {
            let (encoded, inserted_at) = *entry;
            if coarse_now_secs().saturating_sub(inserted_at) < TTL_SECS {
                return Some(decode_source(encoded));
            }
            // Entry is stale — drop the shared ref before removing.
            // Only decrement len if this thread actually performed the remove
            // (another concurrent reader might race to evict the same key).
            drop(entry);
            if self.inner.remove(&key).is_some() {
                self.len.fetch_sub(1, AtomicOrdering::Relaxed);
            }
        }
        None
    }

    /// Store a decision. Accepts a pre-computed key from [`decision_key`].
    #[inline]
    pub fn set_by_key(&self, key: u64, source: Option<BlockSource>) {
        // Bound memory: skip insert when the cache is at capacity.
        // Stale entries will be evicted organically by `get()` reads or
        // by the next `clear()` call on blocklist reload.
        if self.len.load(AtomicOrdering::Relaxed) >= L1_CAPACITY {
            return;
        }
        let value = (encode_source(source), coarse_now_secs());
        match self.inner.entry(key) {
            dashmap::Entry::Vacant(e) => {
                e.insert(value);
                self.len.fetch_add(1, AtomicOrdering::Relaxed);
            }
            dashmap::Entry::Occupied(mut e) => {
                e.insert(value);
            }
        }
    }

    pub fn clear(&self) {
        self.inner.clear();
        self.len.store(0, AtomicOrdering::Relaxed);
    }
}

impl Default for BlockDecisionCache {
    fn default() -> Self {
        Self::new()
    }
}
